---
sort: 6
---

# ConvNeXts  
A ConvNet for the 2020s  

## 1. 개요  
Swin Transformer 같은 Vision Transfomer(ViTs)의 도입으로 다양한 컴퓨터 비전 작업에서 놀라운 성능을 보여주며 백본으로 실질적으로 사용할 수 있음을 보여줬다. 그러나 백본으로 사용하는것과 같이 하이브리드 접근법의 효과는 여전히 Convolution의 고유한 inductive biases보다는 트랜스포머의 intrinsic superiority에 크게 기여한다. 본 논문에서, 모델의 설계 공간을 재검토하고 순수한 ConvNet이 달성할 수 있는 것의 한계를 테스트한다.  

2010년대를 돌아보면, 그 10년은 딥러닝의 기념비적인 발전과 영향에 의해 특징지어졌다. 주요 동인은 신경 네트워크, 특히 컨볼루션 신경망(ConvNets)의 르네상스였다. 10년 동안 시각적 인식 분야는 엔지니어링 기능에서 설계(ConvNet) 아키텍처로 성공적으로 전환되었다. Backward Training ConvNets의 발명은 1980년대까지 거슬러 올라가지만, 우리는 2012년 말에야 시각적 특징 학습의 진정한 잠재력을 보았다. 많은 애플리케이션 시나리오에서 "슬라이딩 윈도우" 전략은 특히 고해상도 이미지로 작업할 때 시각적 처리에 본질적이다. ConvNets에는 다양한 컴퓨터 비전 응용 프로그램에 잘 적합하도록 하는 몇 가지 유도 편향(inductive biases)이 내장되어 있다. 가장 중요한 것은 반대 검출(objection detection)과 같은 작업에 바람직한 속성인 번역 등분산(translation equivariance)이다. ConvNets는 슬라이딩 윈도우 방식으로 사용될 때 계산이 공유되기 때문에 본질적으로 효율적이다.  

비슷한 시기에 트랜스포머가 반복 신경망을 대체하여 지배적인 백본 아키텍처가 되었기 때문에 자연어 처리(NLP)를 위한 신경망 설계의 odyssey는 매우 다른 경로를 택했다. 언어와 비전 도메인 간의 관심 작업의 차이에도 불구하고, 비전 트랜스포머(ViT)의 도입으로 네트워크 아키텍처 설계의 지형이 완전히 바뀌면서 2020년에 두 흐름이 놀랍게도 수렴되었다. 이미지를 일련의 패치로 분할하는 초기 패치 레이어를 제외하고 ViT는 이미지 고유의 inductive biases를 도입하지 않으며 원래의 NLP 트랜스포머에 최소한의 변경을 가한다. ViT의 주요 초점은 확장 동작에 있다. 더 큰 모델 및 데이터 세트 크기의 도움으로 Transformer는 표준 ResNets를 상당한 차이로 능가할 수 있다.  

이미지 분류 작업에 대한 이러한 결과는 고무적이지만 컴퓨터 비전은 이미지 분류에 국한되지 않는다. 가장 큰 과제는 입력 크기에 대해 2차 복잡성을 갖는 ViT의 global attention design이다. 이것은 ImageNet 분류에 허용될 수 있지만 고해상도 입력에서는 빠르게 다루기 어려워진다.  

계층적 변환기(Hierarchical Transformers)는 이러한 격차를 해소하기 위해 하이브리드 접근 방식을 채택한다. 예를 들어, "슬라이딩 윈도우" 전략(예: attention within local windows)이 트랜스포머에 재도입되어 ConvNets와 더 유사하게 동작할 수 있게 되었다. Swin Transformer는 이러한 방향의 획기적인 연구로, Transformer가 일반적인 비전 백본으로 채택되고 이미지 분류를 넘어 다양한 컴퓨터 비전 작업에서 최첨단 성능을 달성할 수 있음을 처음으로 입증한다. Swin Transformer의 성공과 빠른 채택은 컨볼루션의 본질은 무관하게 되는 것을 밝혔다.  

이러한 관점에서 컴퓨터 비전용 트랜스포머의 많은 발전은 컨볼루션을 되살리는 데 목표를 두고 있다. 그러나 이러한 시도는 비용이 든다: sliding window self-attention의 단순한 구현은 비용이 많이 들 수 있다. cyclic shifting과 같은 고급 접근 방식을 사용하면 속도를 최적화할 수 있지만 시스템은 설계에서 더 정교해진다. 반면에, ConvNet이 비록 간단하지만 no-frills 방식으로 이미 원하는 속성 중 많은 것을 만족시킨다는 것은 거의 아이러니하다. ConvNets가 힘을 잃고 있는 것처럼 보이는 유일한 이유는 (계층적) 트랜스포머가 많은 비전 작업에서 트랜스포머를 능가하고, 성능 차이는 대개 멀티헤드 자체 주의가 핵심 구성 요소인 트랜스포머의 우수한 스케일링 동작에 기인한다.  

지난 10년 동안 점진적으로 개선되었던 ConvNets와 달리, 비전 트랜스포머의 채택은 단계적인 변화였다. 최근 문헌에서 시스템 수준 비교(예: Swin Transformer 대 ResNet)는 일반적으로 두 가지를 비교할 때 채택된다. ConvNets와 계층적 비전 트랜스포머는 서로 다르며 동시에 유사해진다. 둘 다 유사한 유도 편향을 갖추고 있지만 훈련 절차와 매크로/마이크로 레벨 아키텍처 설계에서 크게 다르다. 본 논문에서는 ConvNets와 Transformer 사이의 architecture 차이를 조사하고 네트워크 성능을 비교할 때 교란 변수(confounding variables)를 식별하려고 한다. 우리의 연구는 ConvNets를 위한 ViT 이전과 ViT 이후 사이의 격차를 해소하고 순수 ConvNet이 달성할 수 있는 것의 한계를 테스트하기 위한 것이다.  

이를 위해 개선된 절차로 훈련된 표준 ResNet(예: ResNet50)으로 시작한다. 우리는 계층적 비전 트랜스포머(예: Swin-T)의 구축으로 아키텍처를 점진적으로 "현대화"한다. 우리의 탐색은 다음과 같은 핵심 질문에 의해 유도된다. 그 과정에서 성능 차이에 기여하는 몇 가지 핵심 구성 요소를 발견했다. 결과적으로 ConvNeXt라는 순수 ConvNet 제품군을 제안한다. ImageNet 분류, COCO에 대한 객체 감지/세그먼트, ADE20K에 대한 의미 분할과 같은 다양한 비전 작업에 대해 ConvNeXts를 평가한다. 놀랍게도, 전적으로 표준 ConvNet 모듈로 구성된 ConvNeXts는 모든 주요 벤치마크에서 정확성, 확장성 및 견고성 측면에서 Transformer와 유리하게 경쟁한다. ConvNeXt는 표준 ConvNets의 효율성을 유지하며, 훈련과 테스트를 위한 완전한 컨볼루션 특성을 통해 구현이 매우 간단하다.