---
sort: 8
---

# Optimizer  
Optimizer  

## 1. 개요  
개요  

## 2. 경사하강법  
### 1. GD  
경사를 따라 내려가며 W를 업데이트 한다. 딥러닝에서 사용되는 non-linear function은 대부분 닫힌 형태이며, closed form solution이 존재하지 않는다. 존재한다 가정해도 파라미터 개수가 많거나 복잡한 경우가 대부분이기 때문에 경사하강법을 이용하여 효율적으로 계산한다.  

### 2. SGD  
확률적 경사 하강법으로 불린다. 기존 경사하강법은 full-batch 단위로 W값을 업데이트를 진행했지만, SGD는 mini-batch 단위로 업데이트를 진행한다. 이를 통해 W값을 빠르게 업데이트하면서 이전보다 빠르게 수렴할 수 있다.